#ne_tune.ipynb
#Automatically generated by Colab.

#Original file is located at
 #   https://colab.research.google.com/drive/1HaKjPXKKhGCcS84l5_CACoFpsm1STbaO

## YOLO V8 FINE TUNING

#https://medium.com/@amit25173/fine-tuning-yolov8-a-practical-guide-61343dada5c1
#https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb#scrollTo=Y8cDtxLIBHgQ


#!nvidia-smi

#!pip install ultralytics optuna roboflow

from ultralytics import YOLO

from IPython.display import display, Image
import optuna
import os
import optuna
from ultralytics import YOLO
import os
import json
import pandas as pd
import optuna.visualization as vis

# --- Create a central directory for all outputs ---
output_dir = ''
os.makedirs(output_dir, exist_ok=True)
print(f" Created central output directory: {output_dir}")

# --- works fully to train and to test ---
# !pip install ultralytics optuna roboflow

from ultralytics import YOLO
from roboflow import Roboflow
import optuna
import yaml
import os
import shutil
import json

# --- Roboflow: Download Training Dataset ---
rf = Roboflow(api_key="")
train_project = rf.workspace("myworkspace-wzeqx").project("merged_historical_newspaper")
train_dataset_path = os.path.join(output_dir, 'merged_historical_newspaper-1')
train_dataset = train_project.version(1).download("yolov8", location=train_dataset_path)
train_path = train_dataset.location  

# --- Roboflow: Download Test Dataset ---
test_project = rf.workspace("newspaperdata").project("my-first-project-lhuoy")
test_dataset_path = os.path.join(output_dir, 'my-first-project-5')
test_dataset = test_project.version(8).download("yolov8", location=test_dataset_path)
test_yaml_path = test_dataset.location



original_yaml_path = os.path.join(train_path, 'data.yaml')
remapped_yaml_path = os.path.join(train_path, 'remapped_data.yaml')

# --- Capture Original Label Files from Primary Train Folder ---
primary_train_labels_dir = os.path.join(train_path, 'train', 'labels')
original_train_label_files = set(os.listdir(primary_train_labels_dir))

# --- Download Additional Training Dataset (only has train folder) ---
rf = Roboflow(api_key="")
additional_project = rf.workspace("newspaperdata").project("additional_dataset")
additional_path = os.path.join(output_dir, 'additional_dataset-4')
additional_dataset = additional_project.version(4).download("yolov8", location=additional_path)
additional_path = additional_dataset.location  

# --- Merge Additional Dataset (train folder only) into Primary Train Folder ---
import shutil

# Load original class names
with open(original_yaml_path, 'r') as f:
    data_yaml = yaml.safe_load(f)

original_classes = data_yaml['names']
print(f"Original classes: {original_classes}")  # Debug print

# FIXED MAPPING: Directly map numeric class IDs to text (1) or non-text (0)
# Classes 2 (table) and 3 (text) should be mapped to text (1)
# Classes 0 (graphic) and 1 (image) should be mapped to non-text (0)
text_classes = [2, 3]  # These class IDs will be mapped to text (1)
class_mapping = {i: 1 if i in text_classes else 0 for i in range(len(original_classes))}

print(f"Classes that will be mapped to text (1): {text_classes}")
print(f"Classes that will be mapped to non-text (0): {[i for i in range(len(original_classes)) if i not in text_classes]}")
print(f"Final class mapping: {class_mapping}")
data_yaml['nc'] = 2
data_yaml['names'] = {
    0: 'non-text',
    1: 'text'
}


with open(remapped_yaml_path, 'w') as f:
    yaml.dump(data_yaml, f, sort_keys=False)

print(f" Remapped YAML saved to: {remapped_yaml_path}")
print(f" Class mapping: {class_mapping}")
test_yaml_path = os.path.join(test_dataset.location, "data.yaml")

# Load and patch it
with open(test_yaml_path, 'r') as f:
    test_yaml = yaml.safe_load(f)

# Properly assign string paths
test_yaml['train'] = 'test/images'
test_yaml['val'] = 'test/images'
test_yaml['test'] = 'test/images'

# Fix class names if needed
test_yaml['names'] = {
    0: 'non-text',
    1: 'text'
}

# Save fixed YAML
with open(test_yaml_path, 'w') as f:
    yaml.dump(test_yaml, f, sort_keys=False)

print(f" Fixed test data.yaml at: {test_yaml_path}")
# --- Remap Labels in Label Files ---
def remap_labels(labels_dir, class_map):
    for file in os.listdir(labels_dir):
        if file.endswith('.txt'):
            path = os.path.join(labels_dir, file)
            with open(path, 'r') as f:
                lines = f.readlines()

            new_lines = []
            for line in lines:
                parts = line.strip().split()
                if not parts:
                    continue
                original_class = int(parts[0])
                if original_class in class_map:
                    parts[0] = str(class_map[original_class])
                    new_lines.append(' '.join(parts) + '\n')

            with open(path, 'w') as f:
                f.writelines(new_lines)

print("Remapping training labels...")
remap_labels(os.path.join(train_path, 'train/labels'), class_mapping)
remap_labels(os.path.join(train_path, 'valid/labels'), class_mapping)
remap_labels(os.path.join(train_path, 'test/labels'), class_mapping)
print(" Label remapping complete.")

def merge_datasets_train(primary_path, additional_path):
    """
    Merges images and labels from the additional dataset's train folder into the primary dataset's train folder.
    """
    additional_train_path = os.path.join(additional_path, 'train')
    if not os.path.exists(additional_train_path):
        print("No train folder found in additional dataset.")
        return

    primary_train_images_dir = os.path.join(primary_path, 'train', 'images')
    primary_train_labels_dir = os.path.join(primary_path, 'train', 'labels')
    additional_train_images_dir = os.path.join(additional_train_path, 'images')
    additional_train_labels_dir = os.path.join(additional_train_path, 'labels')

    # Merge images from additional dataset
    if os.path.exists(additional_train_images_dir):
        for filename in os.listdir(additional_train_images_dir):
            src_img = os.path.join(additional_train_images_dir, filename)
            dst_img = os.path.join(primary_train_images_dir, filename)
            # Handle potential filename conflicts if needed
            shutil.copy(src_img, dst_img)

    # Merge labels from additional dataset (these do not need remapping)
    if os.path.exists(additional_train_labels_dir):
        for filename in os.listdir(additional_train_labels_dir):
            src_label = os.path.join(additional_train_labels_dir, filename)
            dst_label = os.path.join(primary_train_labels_dir, filename)
            shutil.copy(src_label, dst_label)

merge_datasets_train(train_path, additional_path)
print(" Additional dataset (train folder only) merged into primary training dataset.")
# --- Remap Additional Dataset (my-first-project-3) ---
print(" Starting remapping for my-first-project-3 dataset...")



import os
import shutil
import random

# Define the dataset path (adjust to your folder)
dataset_path = train_path  # Use the updated train_path 

# Define original splits to combine
original_splits = ["train", "valid", "test"]

# Create temporary combined folder
all_images_dir = os.path.join(dataset_path, "all", "images")
all_labels_dir = os.path.join(dataset_path, "all", "labels")
os.makedirs(all_images_dir, exist_ok=True)
os.makedirs(all_labels_dir, exist_ok=True)

# Combine images and labels from train, valid, and test
for split in original_splits:
    split_images_dir = os.path.join(dataset_path, split, "images")
    split_labels_dir = os.path.join(dataset_path, split, "labels")
    
    # If the split folder exists, copy its contents
    if os.path.exists(split_images_dir):
        for img_file in os.listdir(split_images_dir):
            if img_file.lower().endswith((".jpg", ".jpeg", ".png")):
                src_img = os.path.join(split_images_dir, img_file)
                dst_img = os.path.join(all_images_dir, img_file)
                shutil.copy(src_img, dst_img)
                
    if os.path.exists(split_labels_dir):
        for label_file in os.listdir(split_labels_dir):
            if label_file.endswith(".txt"):
                src_label = os.path.join(split_labels_dir, label_file)
                dst_label = os.path.join(all_labels_dir, label_file)
                shutil.copy(src_label, dst_label)

print(" Combined train, valid, and test splits into 'all' folder.")
print(f" Total files in 'all' folder: {len(os.listdir(all_images_dir))} images and {len(os.listdir(all_labels_dir))} labels")

# List all combined images (ensure only image files are taken)
all_images = [f for f in os.listdir(all_images_dir) if f.lower().endswith((".jpg", ".jpeg", ".png"))]

# Shuffle the images for randomness
random.shuffle(all_images)

# Determine the split sizes: 90% train, 10% val
val_count = int(len(all_images) * 0.10)
train_count = len(all_images) - val_count
train_images = all_images[:train_count]
val_images = all_images[train_count:]

print(f"Selected {len(train_images)} images for train and {len(val_images)} images for val.")

# Remove (or rename) the old train and valid folders to avoid mixing
for split in ["train", "valid"]:
    split_path = os.path.join(dataset_path, split)
    if os.path.exists(split_path):
        shutil.rmtree(split_path)
        print(f"Removed old '{split}' folder.")

# Recreate new train and val folder structure
new_train_images_dir = os.path.join(dataset_path, "train", "images")
new_train_labels_dir = os.path.join(dataset_path, "train", "labels")
new_val_images_dir = os.path.join(dataset_path, "valid", "images")
new_val_labels_dir = os.path.join(dataset_path, "valid", "labels")

os.makedirs(new_train_images_dir, exist_ok=True)
os.makedirs(new_train_labels_dir, exist_ok=True)
os.makedirs(new_val_images_dir, exist_ok=True)
os.makedirs(new_val_labels_dir, exist_ok=True)

# Helper function to copy image and its corresponding label
def copy_image_and_label(img_file, dest_images_dir, dest_labels_dir):
    src_img = os.path.join(all_images_dir, img_file)
    dst_img = os.path.join(dest_images_dir, img_file)
    shutil.copy(src_img, dst_img)
    
    # Construct corresponding label filename
    label_file = os.path.splitext(img_file)[0] + ".txt"
    src_label = os.path.join(all_labels_dir, label_file)
    if os.path.exists(src_label):
        dst_label = os.path.join(dest_labels_dir, label_file)
        shutil.copy(src_label, dst_label)

# Copy files into new train folder
for img in train_images:
    copy_image_and_label(img, new_train_images_dir, new_train_labels_dir)

# Copy files into new val folder
for img in val_images:
    copy_image_and_label(img, new_val_images_dir, new_val_labels_dir)

print(" New train and val splits created with the same folder names.")
print(f" Final counts - Train: {len(os.listdir(new_train_images_dir))} images, {len(os.listdir(new_train_labels_dir))} labels | Val: {len(os.listdir(new_val_images_dir))} images, {len(os.listdir(new_val_labels_dir))} labels")


# --- Cleanup previous trials ---
def clean_previous_trials(base_path=os.path.join(output_dir, 'runs/detect')):
    if os.path.exists(base_path):
        for folder in os.listdir(base_path):
            if folder.startswith("optuna_trial_"):
                shutil.rmtree(os.path.join(base_path, folder))
clean_previous_trials()

# --- Define Optuna Objective ---
def objective(trial):
    print(f" Running trial {trial.number}")

    # Expanded learning rate range
    lr = trial.suggest_loguniform('lr0', 1e-6, 1e-3)
    
    # Reduced batch size for larger model to prevent OOM
    batch = trial.suggest_categorical('batch', [4 ,8])
    
    # Added more optimizers
    optimizer = trial.suggest_categorical('optimizer', ['SGD', 'AdamW'])
    
    # Adjusted momentum range
    momentum = trial.suggest_float('momentum', 0.8, 0.95)
    
    # Expanded weight decay range
    weight_decay = trial.suggest_float('weight_decay', 0.0001, 0.01)
    
    # Added warmup epochs
    warmup_epochs = trial.suggest_int('warmup_epochs', 2, 5)
    
    # Added cosine learning rate scheduling
    cos_lr = trial.suggest_categorical('cos_lr', [True, False])

    model = YOLO('yolov8x.pt')  
    run_name = f"optuna_trial_{trial.number:03d}"

    model.train(
        data=remapped_yaml_path,
        epochs=100,
        imgsz=640,
        lr0=lr,
        batch=batch,
        optimizer=optimizer,
        momentum=momentum,
        weight_decay=weight_decay,
        warmup_epochs=warmup_epochs,
        cos_lr=cos_lr,
        name=run_name,
        verbose=False,
        patience=30,  # Early stopping patience
        project=os.path.join(output_dir, 'runs/detect')  # Save runs in yolov8 directory
    )

    metrics = model.val()
    return metrics.box.map  # mAP50

# --- Run Optuna Study ---
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=10)

# --- Load Best Model ---
best_trial = study.best_trial
best_model_path = os.path.join(output_dir, f"runs/detect/optuna_trial_{best_trial.number:03d}/weights/best.pt")
best_model = YOLO(best_model_path)

print(f"\n Best model path: {best_model_path}")

# --- Final Evaluation on Separate Test Set ---
test_metrics = best_model.val(data=test_yaml_path)
print("\n Final Test Set Evaluation:")
for k, v in test_metrics.results_dict.items():
    print(f"{k}: {v}")

# --- Save Test Results ---
test_metrics_path = os.path.join(output_dir, "test_metrics.json")
with open(test_metrics_path, "w") as f:
    json.dump(test_metrics.results_dict, f, indent=4)

print(f" Saved final test results to {test_metrics_path}")
from IPython.display import Image, display
import glob
import os

# Custom directory for saving predictions
prediction_dir = os.path.join(output_dir, 'predictions')
os.makedirs(prediction_dir, exist_ok=True)

# Run prediction on test images
test_images_dir = os.path.join(test_dataset.location, 'test', 'images')

results = best_model.predict(
    source=test_images_dir,
    conf=0.25,
    save=True,
    project=prediction_dir,  # sets custom base directory
    name='run',               
    exist_ok=True             # overwrite if already exists
)

# Get the full path where predictions are saved
save_dir = os.path.join(prediction_dir, 'run')
print(f"\n Predictions saved to: {save_dir}")

# Show a few sample predictions
predicted_images = sorted(glob.glob(f"{save_dir}/*.jpg"))[:5]
print("\n Showing sample predictions:")
for img_path in predicted_images:
    display(Image(filename=img_path))

# --- Copy the final model to directory for easy access ---
final_model_path = os.path.join(output_dir, "best_model.pt")
shutil.copy(best_model_path, final_model_path)
print(f"\n Best model copied to {final_model_path} for easy access")